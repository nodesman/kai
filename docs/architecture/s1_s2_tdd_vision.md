# The S1/S2 TDD Architectural Vision for Reliable Implementation

**Purpose:** This document defines the intended architecture and workflow of Kai's core agentic systems (System 1 & System 2). It explains the rationale behind the design choices, particularly the focus on Test-Driven Development (TDD) to achieve reliable implementation, aligning with the primary goal of maximizing personal leverage.

## Overview of the Two-System Philosophy

Kai's agentic workflow is deliberately split into two distinct systems to enforce a separation of concerns crucial for reliable software generation:

1.  **System 1 (S1): Requirements Clarification & Specification Generation.** Focuses on understanding **WHAT** needs to be built or changed. Its responsibility is to transform potentially ambiguous user intent into a precise, formal plan.
2.  **System 2 (S2): Test-Driven Implementation Execution.** Focuses on **HOW** to implement the plan defined by S1. Its responsibility is the methodical, test-driven generation and verification of code changes.

This separation ensures that implementation (S2) proceeds based on a clear, unambiguous, and validated plan (the `Specification`), rather than attempting to interpret raw user requests directly. This structure aims to significantly enhance the reliability and correctness of the generated code, especially for complex tasks or modifications to existing codebases, directly supporting the goal of high-leverage, high-quality personal software creation.

## System 1: Requirements Clarification & Formal Specification

*   **Primary Responsibility:** To transform a user's request (which might be somewhat ambiguous or incomplete) into a formal, detailed, and testable specification.
*   **Inputs:**
    *   User's initial request (e.g., "Add feature X", "Modify function Y").
    *   Relevant existing code context (provided by `ProjectContextBuilder`).
    *   Potentially, conversation history for clarification steps.
*   **Process (Orchestrated by `RequirementAgentService`):**
    *   **Analysis:** Analyzes the request against the provided code context.
    *   **Clarification:** Engages in a potential back-and-forth Q&A with the user (via `InteractionManager`) to resolve ambiguities, define scope, and gather necessary details. Uses AI (potentially with function calls like `analyze_requirement_completeness`) to identify gaps or inconsistencies.
    *   **Planning & Test Definition:** Breaks down the requirement, identifies necessary code changes/components, and crucially, defines **test scenarios** (unit, integration, edge cases) that will verify the implementation (e.g., using `generate_test_scenarios` AI task).
*   **Output: The `Specification` Object:**
    *   S1's final output is a structured data object, the `Specification`. This artifact is critical as it serves as the immutable contract for S2.
    *   **Structure:** Contains details like feature description, affected files/modules, detailed description of required changes (logic, signatures), data models/APIs (if applicable), non-functional requirements, and the essential list of **Test Scenarios / Skeletons** (e.g., `{ description: "Test VAT calculation", type: 'unit', focusArea: 'billing.calculateTotal' }`).
*   **Key Distinction:** System 1 **does not** write the final implementation code. It defines *what* needs to be done and *how to test it*.

## System 2: Test-Driven Development (TDD) Implementation Execution

*   **Primary Responsibility:** To take the `Specification` from System 1 and implement the required changes using a rigorous Test-Driven Development (TDD) cycle.
*   **Inputs:**
    *   The immutable `Specification` object generated by S1.
    *   The current state of the codebase.
*   **Core TDD Loop (Orchestrated by `AgenticTddService`):**
    1.  **Select Test:** Choose the next test scenario from the `Specification`.
    2.  **Generate Test Code:** Use AI (`generate_test_code` prompt) to write the actual test code based on the scenario description.
    3.  **Run Test:** Execute the newly generated test using `TestRunnerService`. *Expect failure* against the current (unmodified or partially modified) code.
    4.  **Analyze Failure:** Use AI (`diagnose_test_failure` prompt), analyzing the specific failure output (parsed errors, stack traces via Test Output Parsers) in the context of the `Specification` and relevant code snippets.
    5.  **Generate Code Fix (Diff):** Use AI (`generate_code_fix_or_impl` prompt) to generate the **minimal code change (ideally a diff patch)** required to make *that specific test* pass, based on the failure analysis and the `Specification`.
    6.  **Apply Change:** Apply the generated diff/code using `FileSystem.applyDiffToFile`.
    7.  **Re-run Test:** Execute the *same* test again. *Expect pass*. If it fails, potentially loop back to analysis/generation with more context or trigger human review (future enhancement).
    8.  **Repeat:** Continue the loop until all test scenarios defined in the `Specification` are implemented and pass.
*   **Handling Modifications:** This loop is inherently well-suited for modifying existing code safely. By focusing on targeted test failures and generating minimal diffs, it reduces the risk of regressions compared to generating entire files.

## Why TDD is Central to the Vision

The Test-Driven Development approach within System 2 is not merely an implementation detail; it is fundamental to achieving Kai's goal of reliable AI-driven implementation:

*   **Ensures Verifiability:** Code is generated specifically to satisfy concrete, testable requirements derived directly from the clarified `Specification`. Each incremental change is validated by a passing test.
*   **Manages Complexity:** Breaks down potentially large or complex features into smaller, manageable, independently verifiable steps (passing one test at a time).
*   **Promotes Correctness:** Reduces the likelihood of bugs or regressions introduced during generation, as each step is validated before moving on.
*   **Contrast with Alternatives:** This contrasts sharply with:
    *   **Direct Generation:** Where AI generates code directly from a prompt without intermediate verification, risking subtle errors or incomplete implementations.
    *   **Consolidation Mode:** Which interprets broader conversation history to generate a final file state, lacking the step-by-step verification and targeted diff approach for modifications, making it potentially less reliable for complex changes (see `docs/explanations/s1s2-vs-consolidation-modifications.md`).

## The Technical Bet

The specific S1 -> `Specification` -> S2 (TDD) architecture is Kai's core technical hypothesis. It represents a bet that this structured, verifiable, test-driven approach will prove significantly more reliable and effective for generating and modifying complex, high-quality software with AI assistance compared to less structured, direct-generation methods. Successfully implementing this vision is the primary technical objective and the foundation of Kai's potential unique advantage as a tool for high-leverage personal software creation.